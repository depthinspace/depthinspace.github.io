<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="description" content="DepthInSpace: Exploitation and Fusion of Multiple Video Frames for Structured-Light Depth Estimation">
    <meta name="keywords" content="DepthInSpace">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>DepthInSpace: Exploitation and Fusion of Multiple Video Frames for Structured-Light Depth Estimation</title>
 
    <script>
      window.dataLayer = window.dataLayer || [];
      
      function gtag() {
      dataLayer.push(arguments);
      }

      gtag('js', new Date());

      gtag('config', 'G-PYVRSFMDRL');
    </script>

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="stylesheet" type="text/css" href="./static/css/magnifier.css">
    
  </head>

  <body>

    <section class="hero">
      <div class="hero-body">
        <div class="container">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-2 publication-title">DepthInSpace: Exploitation and Fusion of Multiple Video Frames for Structured-Light Depth Estimation</h1>
              <div class="is-size-5 publication-authors">
                
                <span class="author-block">
                  <a href="https://people.epfl.ch/mohammad.johari/?lang=en"> Mohammad Mahdi Johari</a> <sup>1,2</sup>, <a href="https://ch.linkedin.com/in/camilla-carta"> Camilla Carta </a><sup>3</sup>, <a href="https://fleuret.org/francois/"> Fran√ßois Fleuret </a><sup>4,2,1</sup> </span>

                <div class="is-size-5 publication-authors">
                  <span class="author-block"><sup>1</sup> Idiap Research Institute &nbsp;&nbsp;</span>
                  <span class="author-block"><sup>2</sup> EPFL &nbsp;&nbsp;</span>
                  <span class="author-block"><sup>3</sup> ams Osram &nbsp;&nbsp;</span>
                  <span class="author-block"><sup>4</sup> University of Geneva &nbsp;&nbsp;</span>
                </div>
                
                <h1 style="font-size:24px;font-weight:bold">ICCV 2021</h1>
                <div class="column has-text-centered">
                  <div class="publication-links">
                    
                    <!-- <\!-- Paper -\->               -->
                    <span class="link-block">
                      <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Johari_DepthInSpace_Exploitation_and_Fusion_of_Multiple_Video_Frames_for_Structured-Light_ICCV_2021_paper.pdf"
                         class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- <\!-- Supplementary -\->               -->                    
                    <span class="link-block">
                      <a href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Johari_DepthInSpace_Exploitation_and_ICCV_2021_supplemental.pdf"
                         class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Supplementary</span>
                      </a>
                    </span>

                    <!-- <\!-- Code -\->               -->                    
                    <span class="link-block">
                      <a href="https://github.com/idiap/DepthInSpace"
                         class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                      </a>
                    </span>

                    <!-- <\!-- Data -\->               -->                    
                    <span class="link-block">
                      <a href="https://www.idiap.ch/en/dataset/depthinspace/index_html"
                         class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-database"></i>
                        </span>
                        <span>Data</span>
                      </a>
                    </span>
                   
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
    </section>
    
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe width="684" height="385" src="https://www.youtube.com/embed/f4LEvIjxKSc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
      </div>
    </div>

        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-two-thirds">
            <h2 class="title is-2">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                We present DepthInSpace, a self-supervised deep-learning method for depth estimation using a structured-light camera. The design of this method is motivated by the commercial use case of embedded depth sensors in nowadays smartphones. We first propose to use estimated optical flow from ambient information of multiple video frames as a complementary guide for training a single-frame depth estimation network, helping to preserve edges and reduce over-smoothing issues. Utilizing optical flow, we also propose to fuse the data of multiple video frames to get a more accurate depth map. In particular, fused depth maps are more robust in occluded areas and incur less in flying pixels artifacts. We finally demonstrate that these more precise fused depth maps can be used as self-supervision for fine-tuning a single-frame depth estimation network to improve its performance. Our models' effectiveness is evaluated and compared with state-of-the-art models on both synthetic and our newly introduced real datasets.
              </p>

            </div>
          </div>
        </div>
      </div>
    </section>
      
      <section class="section" id="BibTeX citation">
        <div class="container content">
          <h2 class="title">BibTeX</h2>
          <pre><code>
            @inproceedings{johari-et-al-2021,
              author = {Johari, M. and Carta, C. and Fleuret, F.},
              title = {DepthInSpace: Exploitation and Fusion of Multiple Video Frames for Structured-Light Depth Estimation},
              booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
              year = {2021}
            }
          </code></pre>
        </div>
      </section>
      
  </body>
</html>
